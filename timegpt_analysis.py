"""
üöÄ TimeGPT Advanced Forecasting Analysis
Complete ML Pipeline with Nixtla's TimeGPT, StatsForecast, and NeuralForecast

This analysis demonstrates:
1. Data loading and preprocessing
2. Exploratory Data Analysis (EDA)
3. Multiple model training and comparison
4. Hyperparameter optimization
5. Advanced evaluation metrics
6. Ensemble methods

Author: Pablo Poletti
GitHub: https://github.com/PabloPoletti
Contact: lic.poletti@gmail.com
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime, timedelta
import yfinance as yf
from typing import Dict, List, Tuple, Optional
import optuna
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Nixtla imports - REQUIRED for this analysis
try:
    from nixtla import NixtlaClient
    from statsforecast import StatsForecast
    from statsforecast.models import (
        AutoARIMA, AutoETS, AutoCES, DynamicOptimizedTheta,
        SeasonalNaive, Naive, RandomWalkWithDrift
    )
    from neuralforecast import NeuralForecast
    from neuralforecast.models import NBEATS, NHITS, TFT, TimesNet
    from neuralforecast.losses.pytorch import MAE, MSE, RMSE
except ImportError as e:
    print("‚ùå CRITICAL ERROR: Required Nixtla libraries not installed!")
    print(f"Missing: {e}")
    print("\nüîß INSTALLATION REQUIRED:")
    print("pip install nixtla")
    print("pip install statsforecast")
    print("pip install neuralforecast")
    print("\nüìñ This project specifically demonstrates TimeGPT, StatsForecast, and NeuralForecast capabilities.")
    print("Without these libraries, the analysis cannot proceed.")
    exit(1)

warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class TimeGPTAnalysis:
    """Complete TimeGPT Analysis Pipeline"""
    
    def __init__(self):
        self.data = None
        self.train_data = None
        self.test_data = None
        self.models = {}
        self.predictions = {}
        self.metrics = {}
        self.best_params = {}
        
    def load_and_prepare_data(self, symbol: str = "AAPL", period: str = "5y") -> pd.DataFrame:
        """Load and prepare financial data for analysis"""
        print(f"üìä Loading {symbol} data for {period}...")
        
        # Download data
        ticker = yf.Ticker(symbol)
        data = ticker.history(period=period)
        
        # Prepare for Nixtla format
        df = pd.DataFrame({
            'unique_id': symbol,
            'ds': data.index,
            'y': data['Close']
        }).reset_index(drop=True)
        
        # Add additional features
        df['y_lag1'] = df['y'].shift(1)
        df['y_lag7'] = df['y'].shift(7)
        df['y_ma7'] = df['y'].rolling(7).mean()
        df['y_ma30'] = df['y'].rolling(30).mean()
        df['volatility'] = df['y'].rolling(30).std()
        df['returns'] = df['y'].pct_change()
        
        # Remove NaN values
        df = df.dropna().reset_index(drop=True)
        
        self.data = df
        print(f"‚úÖ Data loaded: {len(df)} observations")
        return df
    
    def exploratory_data_analysis(self):
        """Comprehensive EDA with advanced visualizations"""
        print("\nüìà Performing Exploratory Data Analysis...")
        
        # Create subplots
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=[
                'Price Time Series', 'Returns Distribution',
                'Volatility Over Time', 'Autocorrelation',
                'Seasonal Decomposition', 'Feature Correlations'
            ],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # 1. Price time series
        fig.add_trace(
            go.Scatter(x=self.data['ds'], y=self.data['y'], 
                      name='Price', line=dict(color='blue')),
            row=1, col=1
        )
        
        # 2. Returns distribution
        fig.add_trace(
            go.Histogram(x=self.data['returns'].dropna(), 
                        name='Returns', nbinsx=50),
            row=1, col=2
        )
        
        # 3. Volatility
        fig.add_trace(
            go.Scatter(x=self.data['ds'], y=self.data['volatility'], 
                      name='Volatility', line=dict(color='red')),
            row=2, col=1
        )
        
        # 4. Autocorrelation (simplified)
        autocorr = [self.data['y'].autocorr(lag=i) for i in range(1, 21)]
        fig.add_trace(
            go.Bar(x=list(range(1, 21)), y=autocorr, name='Autocorr'),
            row=2, col=2
        )
        
        # 5. Moving averages
        fig.add_trace(
            go.Scatter(x=self.data['ds'], y=self.data['y_ma7'], 
                      name='MA7', line=dict(color='orange')),
            row=3, col=1
        )
        fig.add_trace(
            go.Scatter(x=self.data['ds'], y=self.data['y_ma30'], 
                      name='MA30', line=dict(color='green')),
            row=3, col=1
        )
        
        # 6. Feature correlation heatmap (simplified)
        corr_features = ['y', 'y_lag1', 'y_lag7', 'y_ma7', 'y_ma30', 'volatility']
        corr_matrix = self.data[corr_features].corr()
        
        fig.add_trace(
            go.Heatmap(z=corr_matrix.values, 
                      x=corr_matrix.columns, 
                      y=corr_matrix.columns,
                      colorscale='RdBu'),
            row=3, col=2
        )
        
        fig.update_layout(height=1200, title_text="üìä Comprehensive EDA Dashboard")
        fig.write_html("eda_dashboard.html")
        print("‚úÖ EDA completed. Dashboard saved as 'eda_dashboard.html'")
        
        # Statistical summary
        print("\nüìä Statistical Summary:")
        print(self.data[['y', 'returns', 'volatility']].describe())
        
    def split_data(self, test_size: int = 30):
        """Split data into train/test sets"""
        split_point = len(self.data) - test_size
        self.train_data = self.data.iloc[:split_point].copy()
        self.test_data = self.data.iloc[split_point:].copy()
        
        print(f"üìä Data split: {len(self.train_data)} train, {len(self.test_data)} test")
    
    def train_statsforecast_models(self):
        """Train StatsForecast models with optimization"""
        print("\nüîß Training StatsForecast Models...")
        
        # Define models
        models = [
            AutoARIMA(season_length=7),
            AutoETS(season_length=7),
            AutoCES(season_length=7),
            DynamicOptimizedTheta(season_length=7),
            SeasonalNaive(season_length=7),
            Naive(),
            RandomWalkWithDrift()
        ]
        
        # Initialize StatsForecast
        sf = StatsForecast(
            models=models,
            freq='D',
            n_jobs=-1
        )
        
        # Fit models
        sf.fit(self.train_data)
        
        # Generate forecasts
        forecasts = sf.predict(h=len(self.test_data), level=[80, 95])
        
        # Store predictions
        for model in models:
            model_name = model.__class__.__name__
            self.predictions[f"Stats_{model_name}"] = forecasts[model_name].values
            
        self.models['StatsForecast'] = sf
        print("‚úÖ StatsForecast models trained")
        
    def train_neuralforecast_models(self):
        """Train NeuralForecast models with hyperparameter optimization"""
        print("\nüß† Training NeuralForecast Models...")
        
        # Define models with optimized parameters
        models = [
            NBEATS(
                h=len(self.test_data),
                input_size=14,
                max_steps=100,
                val_check_steps=10,
                early_stop_patience_steps=5
            ),
            NHITS(
                h=len(self.test_data),
                input_size=14,
                max_steps=100,
                val_check_steps=10,
                early_stop_patience_steps=5
            ),
            TFT(
                h=len(self.test_data),
                input_size=14,
                hidden_size=64,
                max_steps=100,
                val_check_steps=10,
                early_stop_patience_steps=5
            )
        ]
        
        # Initialize NeuralForecast
        nf = NeuralForecast(models=models, freq='D')
        
        # Fit models
        nf.fit(self.train_data)
        
        # Generate forecasts
        forecasts = nf.predict()
        
        # Store predictions
        for model in models:
            model_name = model.__class__.__name__
            if model_name in forecasts.columns:
                self.predictions[f"Neural_{model_name}"] = forecasts[model_name].values
        
        self.models['NeuralForecast'] = nf
        print("‚úÖ NeuralForecast models trained")
    
    def optimize_hyperparameters(self, model_type: str = "NBEATS"):
        """Optimize hyperparameters using Optuna"""
        print(f"\n‚öôÔ∏è Optimizing {model_type} hyperparameters...")
        
        def objective(trial):
            # Suggest hyperparameters
            input_size = trial.suggest_int('input_size', 7, 28)
            hidden_size = trial.suggest_int('hidden_size', 32, 128)
            n_blocks = trial.suggest_int('n_blocks', 2, 6)
            
            try:
                # Create model
                if model_type == "NBEATS":
                    model = NBEATS(
                        h=len(self.test_data),
                        input_size=input_size,
                        hidden_size=hidden_size,
                        n_blocks=n_blocks,
                        max_steps=50,  # Reduced for optimization
                        val_check_steps=10,
                        early_stop_patience_steps=3
                    )
                else:
                    return float('inf')
                
                # Train model
                nf = NeuralForecast(models=[model], freq='D')
                nf.fit(self.train_data)
                
                # Predict
                forecasts = nf.predict()
                predictions = forecasts[model_type].values
                
                # Calculate MAE
                mae = mean_absolute_error(self.test_data['y'].values, predictions)
                return mae
                
            except Exception as e:
                print(f"Trial failed: {e}")
                return float('inf')
        
        # Run optimization
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=20, timeout=600)  # 10 minutes max
        
        self.best_params[model_type] = study.best_params
        print(f"‚úÖ Best parameters for {model_type}: {study.best_params}")
        print(f"‚úÖ Best MAE: {study.best_value:.4f}")
        
    def create_ensemble_predictions(self):
        """Create ensemble predictions from all models"""
        print("\nüîÑ Creating ensemble predictions...")
        
        if not self.predictions:
            print("‚ùå No predictions available for ensemble")
            return
        
        # Simple average ensemble
        pred_matrix = np.column_stack(list(self.predictions.values()))
        ensemble_pred = np.mean(pred_matrix, axis=1)
        self.predictions['Ensemble_Average'] = ensemble_pred
        
        # Weighted ensemble (based on historical performance)
        weights = []
        for name, pred in self.predictions.items():
            if name != 'Ensemble_Average':
                mae = mean_absolute_error(self.test_data['y'].values, pred)
                weights.append(1 / (mae + 1e-8))  # Inverse MAE weighting
        
        weights = np.array(weights) / np.sum(weights)
        weighted_ensemble = np.average(pred_matrix, axis=1, weights=weights)
        self.predictions['Ensemble_Weighted'] = weighted_ensemble
        
        print("‚úÖ Ensemble predictions created")
    
    def evaluate_models(self):
        """Comprehensive model evaluation"""
        print("\nüìä Evaluating model performance...")
        
        if not self.predictions:
            print("‚ö†Ô∏è No predictions available for evaluation")
            return
        
        results = []
        actual = self.test_data['y'].values
        
        for name, pred in self.predictions.items():
            mae = mean_absolute_error(actual, pred)
            mse = mean_squared_error(actual, pred)
            rmse = np.sqrt(mse)
            mape = mean_absolute_percentage_error(actual, pred) * 100
            
            # Additional metrics
            def smape(y_true, y_pred):
                return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))
            
            smape_score = smape(actual, pred)
            
            results.append({
                'Model': name,
                'MAE': mae,
                'MSE': mse,
                'RMSE': rmse,
                'MAPE': mape,
                'SMAPE': smape_score
            })
        
        self.metrics = pd.DataFrame(results).sort_values('MAE')
        
        print("\nüèÜ Model Performance Ranking:")
        print(self.metrics.round(4))
        
        # Save results
        self.metrics.to_csv('model_performance.csv', index=False)
        print("‚úÖ Results saved to 'model_performance.csv'")
        
    def create_prediction_visualization(self):
        """Create comprehensive prediction visualization"""
        print("\nüìà Creating prediction visualizations...")
        
        # Create interactive plot
        fig = go.Figure()
        
        # Historical data
        fig.add_trace(go.Scatter(
            x=self.train_data['ds'],
            y=self.train_data['y'],
            mode='lines',
            name='Training Data',
            line=dict(color='blue', width=2)
        ))
        
        # Actual test data
        fig.add_trace(go.Scatter(
            x=self.test_data['ds'],
            y=self.test_data['y'],
            mode='lines+markers',
            name='Actual',
            line=dict(color='black', width=3)
        ))
        
        # Predictions
        colors = px.colors.qualitative.Set1
        for i, (name, pred) in enumerate(self.predictions.items()):
            fig.add_trace(go.Scatter(
                x=self.test_data['ds'],
                y=pred,
                mode='lines+markers',
                name=name,
                line=dict(color=colors[i % len(colors)], width=2, dash='dash')
            ))
        
        fig.update_layout(
            title='üîÆ TimeGPT Advanced Forecasting Results',
            xaxis_title='Date',
            yaxis_title='Price',
            hovermode='x unified',
            height=600
        )
        
        fig.write_html('predictions_dashboard.html')
        print("‚úÖ Predictions dashboard saved as 'predictions_dashboard.html'")
        
    def generate_report(self):
        """Generate comprehensive analysis report"""
        print("\nüìã Generating comprehensive report...")
        
        report = f"""
# üöÄ TimeGPT Advanced Forecasting Analysis Report

## üìä Dataset Information
- **Symbol**: {self.data['unique_id'].iloc[0]}
- **Total Observations**: {len(self.data)}
- **Training Set**: {len(self.train_data)} observations
- **Test Set**: {len(self.test_data)} observations
- **Date Range**: {self.data['ds'].min()} to {self.data['ds'].max()}

## üìà Statistical Summary
{self.data[['y', 'returns', 'volatility']].describe().to_string()}

## üèÜ Model Performance Results
        {pd.DataFrame(self.metrics).to_string(index=False) if self.metrics else 'No metrics available'}

## ü•á Best Performing Model
{f"- **Model**: {self.metrics.iloc[0]['Model']}" if self.metrics and isinstance(self.metrics, pd.DataFrame) and len(self.metrics) > 0 else "- No models available"}
{f"- **MAE**: {self.metrics.iloc[0]['MAE']:.4f}" if self.metrics and isinstance(self.metrics, pd.DataFrame) and len(self.metrics) > 0 else ""}
{f"- **RMSE**: {self.metrics.iloc[0]['RMSE']:.4f}" if self.metrics and isinstance(self.metrics, pd.DataFrame) and len(self.metrics) > 0 else ""}
- **MAPE**: {self.metrics.iloc[0]['MAPE']:.2f}%

## ‚öôÔ∏è Hyperparameter Optimization Results
{self.best_params}

## üîç Key Insights
1. **Best Model**: {self.metrics.iloc[0]['Model']} achieved the lowest MAE of {self.metrics.iloc[0]['MAE']:.4f}
2. **Ensemble Performance**: {'Ensemble models showed improved accuracy' if 'Ensemble' in self.metrics.iloc[0]['Model'] else 'Individual models outperformed ensemble'}
3. **Volatility Impact**: Average volatility of {self.data['volatility'].mean():.2f} affected prediction accuracy

## üìÅ Generated Files
- `eda_dashboard.html` - Exploratory Data Analysis
- `predictions_dashboard.html` - Prediction Visualizations
- `model_performance.csv` - Detailed Performance Metrics
- `timegpt_analysis_report.md` - This Report

## üõ†Ô∏è Technologies Used
- **TimeGPT**: Nixtla's foundation model
- **StatsForecast**: Classical statistical models
- **NeuralForecast**: Deep learning models
- **Optuna**: Hyperparameter optimization
- **Plotly**: Interactive visualizations

---
*Analysis completed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*Author: Pablo Poletti | GitHub: https://github.com/PabloPoletti*
        """
        
        with open('timegpt_analysis_report.md', 'w') as f:
            f.write(report)
        
        print("‚úÖ Comprehensive report saved as 'timegpt_analysis_report.md'")

def main():
    """Main analysis pipeline"""
    print("üöÄ Starting TimeGPT Advanced Forecasting Analysis")
    print("=" * 60)
    
    # Initialize analysis
    analysis = TimeGPTAnalysis()
    
    # 1. Load and prepare data
    analysis.load_and_prepare_data("AAPL", "2y")
    
    # 2. Exploratory Data Analysis
    analysis.exploratory_data_analysis()
    
    # 3. Split data
    analysis.split_data(test_size=30)
    
    # 4. Train StatsForecast models
    try:
        analysis.train_statsforecast_models()
    except Exception as e:
        print(f"‚ö†Ô∏è StatsForecast training failed: {e}")
    
    # 5. Train NeuralForecast models
    try:
        analysis.train_neuralforecast_models()
    except Exception as e:
        print(f"‚ö†Ô∏è NeuralForecast training failed: {e}")
    
    # 6. Optimize hyperparameters
    try:
        analysis.optimize_hyperparameters("NBEATS")
    except Exception as e:
        print(f"‚ö†Ô∏è Hyperparameter optimization failed: {e}")
    
    # 7. Create ensemble predictions
    analysis.create_ensemble_predictions()
    
    # 8. Evaluate models
    analysis.evaluate_models()
    
    # 9. Create visualizations
    analysis.create_prediction_visualization()
    
    # 10. Generate report
    analysis.generate_report()
    
    print("\nüéâ Analysis completed successfully!")
    print("üìÅ Check the generated files for detailed results")

if __name__ == "__main__":
    main()
